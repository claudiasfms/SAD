{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4ºTPC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiasfms/SAD/blob/master/4%C2%BATPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TrQTIrLtAtLR",
        "colab_type": "code",
        "outputId": "936716c9-0b25-49ef-9d04-3300e25d997d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "cell_type": "code",
      "source": [
        "#Cláudia Santos nº20160291\n",
        "#Entrega dia 3 Dezembro\n",
        "\n",
        "#Aplicar Algoritmos\n",
        "#Usando sklearn correr os métodos:\n",
        "#-> Decision Tree (aula 7)\n",
        "#-> Random Forrest  (aula 7)\n",
        "#-> Naive Bayes (aula 9)\n",
        "\n",
        "# para o dataset Digits, definido em baixo;\n",
        "# Usar o training set para executar o treino do modelo;\n",
        "# Comparar o erro obtido em cada método, para o testset e para o training set e expecificar se os valores são os esperados;\n",
        "# Para um dos algoritmos, dar exemplos do test set de instâncias mal bem classificadas (2 de cada);\n",
        "\n",
        "# Decision Trees \n",
        "# Método hierárquico constituído por nós e conexões c/ uma direção, que é alvo de ordenação.\n",
        "\n",
        "# Import datasets, classifiers and performance metrics\n",
        "from sklearn import datasets, tree, model_selection\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "##Random Forrest\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn import datasets, tree,model_selection\n",
        "##Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# The digits dataset\n",
        "digits = datasets.load_digits()\n",
        "digits\n",
        "\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "\n",
        "[features_train, features_test, classes_train, classes_test] = model_selection.train_test_split(digits.data,digits.target, test_size=0.30)\n",
        "model = tree.DecisionTreeClassifier()\n",
        "\n",
        "clf = model.fit(features_train, classes_train)\n",
        "\n",
        "score_train = model.score(features_train, classes_train)\n",
        "score_test = model.score(features_test, classes_test)\n",
        "print (\"Decision Tree\")\n",
        "print(\"Features:\", digits.target_names)\n",
        "print(\"score_train:\", score_train)\n",
        "print(\"score_test:\", score_test)\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "#Random forrest \n",
        "# Método \"Ensemble\" para decision trees- permite tornar modelos fortes a partir de modelos fracos \n",
        "# Obtêm-se amostras aleatórias com repetição de registos do data set de treino, \n",
        "# e sobre cada uma das amostras é gerada uma árvore de decisão\n",
        "\n",
        "[features_train, features_test, classes_train, classes_test] = model_selection.train_test_split(digits.data, digits.target, test_size=0.30)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "\n",
        "clf = model.fit(features_train, classes_train)\n",
        "\n",
        "score_train = model.score(features_train, classes_train)\n",
        "score_test = model.score(features_test, classes_test)\n",
        "print (\"Random Forrest\")\n",
        "print(\"Features:\", digits.target_names)\n",
        "print(\"score_train:\", score_train)\n",
        "print(\"score_test:\", score_test)\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "# Naive Bayes\n",
        "# Este método baseia-se no cálculo de probabilidades condicional \"Teorema de Bayes\"\n",
        "\n",
        "gnb=GaussianNB()\n",
        "\n",
        "[features_train, features_test, classes_train, classes_test] = model_selection.train_test_split(digits.data, digits.target, test_size=0.30)\n",
        "y_pred = gnb.fit(features_train, classes_train)\n",
        "\n",
        "score_train = gnb.score(features_train, classes_train)\n",
        "score_test = gnb.score(features_test, classes_test)\n",
        "\n",
        "print(\"Naive Bayes\")\n",
        "print(\"Features:\", digits.target_names)\n",
        "print(\"score_train:\", score_train)\n",
        "print(\"score_test:\", score_test)\n",
        "print(\"--------------------------------------------\")\n",
        "\n",
        "\n",
        "#Comparando os 3 métodos(Decision Tree;Random Forrest;Naive Bayes), os melhores são o Random Forrest e o Naive Bayes\n",
        "#Sendo o menos preciso e mais demorado o Decision Tree.\n",
        "\n",
        "\n",
        "#Decision Tree\n",
        "#('Features:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
        "#('score_train:', 1.0)\n",
        "#('score_test:', 0.8685185185185185)\n",
        "#--------------------------------------------\n",
        "#Random Forrest\n",
        "#('Features:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
        "#('score_train:', 1.0)\n",
        "#('score_test:', 0.987037037037037)\n",
        "#--------------------------------------------\n",
        "#Naive Bayes\n",
        "#('Features:', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
        "#('score_train:', 0.8933969769291965)\n",
        "#('score_test:', 0.8611111111111112)\n",
        "#--------------------------------------------\n",
        "\n",
        "# O Random Forrest é baseado no Decision Tree, é um método mais evoluído tecnicamente e robusto, permite \n",
        "# obter uma maior precisão uma vez que agrega várias árvores de decisão p/ limitar o \"overfitting\" e erros, melhorando os resultados esperados.\n",
        "\n",
        "# O Naive Bayes quando comparado com o Random Forrest têm em comum uma boa precisão, embora o Random Forrest\n",
        "# consiga ser superior a nível de performance e precisão em universos de grande dimensão.\n",
        "# Neste caso em análise podemos ver que tem uma grande precisão dado estarmos a analisar o \"dataset Digits\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree\n",
            "Features: [0 1 2 3 4 5 6 7 8 9]\n",
            "score_train: 1.0\n",
            "score_test: 0.8722222222222222\n",
            "--------------------------------------------\n",
            "Random Forrest\n",
            "Features: [0 1 2 3 4 5 6 7 8 9]\n",
            "score_train: 1.0\n",
            "score_test: 0.9814814814814815\n",
            "--------------------------------------------\n",
            "Naive Bayes\n",
            "Features: [0 1 2 3 4 5 6 7 8 9]\n",
            "score_train: 0.862370723945903\n",
            "score_test: 0.8166666666666667\n",
            "--------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}